//
//  CoreMLImageProcessor.swift
//  DungeonStat
//
//  Created by Claude on 2025/7/14.
//

import Foundation
import CoreML
import Vision
import UIKit
import Combine
import VideoToolbox

enum ImageProcessingType: String, CaseIterable {
    case animeToSketch = "AnimeToSketch"
    
    var displayName: String {
        switch self {
        case .animeToSketch:
            return "è½¬æ¢ä¸ºç´ æ"
        }
    }
    
    var description: String {
        switch self {
        case .animeToSketch:
            return "å°†åŠ¨æ¼«é£æ ¼å›¾åƒè½¬æ¢ä¸ºçº¿æ¡ç´ æ"
        }
    }
}

struct SketchProcessingParams {
    var contrast: Float = 1.0           // å¯¹æ¯”åº¦ (0.5 - 2.0)
    var brightness: Float = 0.0         // äº®åº¦ (-0.5 - 0.5)
    var lineIntensity: Float = 1.0      // çº¿æ¡å¼ºåº¦ (0.5 - 2.0)
    var backgroundWhiteness: Float = 0.9 // èƒŒæ™¯ç™½åŒ–ç¨‹åº¦ (0.0 - 1.0)
    var enablePreprocessing: Bool = true // å¯ç”¨æ™ºèƒ½é¢„å¤„ç†
    
    static let `default` = SketchProcessingParams()
    
    // é¢„è®¾é…ç½®ï¼ˆå·²é’ˆå¯¹æ™ºèƒ½é¢„å¤„ç†ä¼˜åŒ–ï¼‰
    static let presets: [SketchPreset] = [
        SketchPreset(
            name: "æ™ºèƒ½é»˜è®¤",
            description: "è‡ªé€‚åº”å¤„ç†ï¼Œé€‚ç”¨äºå¤§éƒ¨åˆ†ç…§ç‰‡",
            params: SketchProcessingParams(contrast: 1.0, brightness: 0.0, lineIntensity: 1.0, backgroundWhiteness: 0.85)
        ),
        SketchPreset(
            name: "äººåƒä¸“ç”¨",
            description: "ä¸“ä¸ºäººç‰©ç…§ç‰‡è®¾è®¡ï¼Œå¼ºåŒ–é¢éƒ¨è½®å»“",
            params: SketchProcessingParams(contrast: 1.1, brightness: 0.05, lineIntensity: 1.1, backgroundWhiteness: 0.7)
        ),
        SketchPreset(
            name: "ç»†èŠ‚å¢å¼º",
            description: "æœ€å¤§åŒ–ä¿ç•™ç»†èŠ‚ï¼Œå‡å°‘ç©ºç™½åŒºåŸŸ",
            params: SketchProcessingParams(contrast: 1.2, brightness: 0.0, lineIntensity: 1.3, backgroundWhiteness: 0.6)
        ),
        SketchPreset(
            name: "è‰ºæœ¯é£æ ¼",
            description: "ç”Ÿæˆæ›´å…·è‰ºæœ¯æ„Ÿçš„ç´ ææ•ˆæœ",
            params: SketchProcessingParams(contrast: 0.9, brightness: -0.05, lineIntensity: 0.9, backgroundWhiteness: 0.9)
        ),
        SketchPreset(
            name: "é«˜æ¸…é”åŒ–",
            description: "å¼ºåŒ–çº¿æ¡æ¸…æ™°åº¦å’Œè¾¹ç¼˜å®šä¹‰",
            params: SketchProcessingParams(contrast: 1.3, brightness: -0.1, lineIntensity: 1.5, backgroundWhiteness: 0.75)
        ),
        SketchPreset(
            name: "æŸ”å’Œè‡ªç„¶",
            description: "è‡ªç„¶æŸ”å’Œçš„ç´ æé£æ ¼",
            params: SketchProcessingParams(contrast: 0.8, brightness: 0.1, lineIntensity: 0.8, backgroundWhiteness: 0.95)
        )
    ]
}

struct SketchPreset: Identifiable {
    let id = UUID()
    let name: String
    let description: String
    let params: SketchProcessingParams
}

enum ImageProcessingError: Error, LocalizedError {
    case modelNotFound(String)
    case invalidInput
    case processingFailed
    case memoryError
    
    var errorDescription: String? {
        switch self {
        case .modelNotFound(let modelName):
            return "æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: \(modelName)"
        case .invalidInput:
            return "è¾“å…¥å›¾åƒæ ¼å¼æ— æ•ˆ"
        case .processingFailed:
            return "å›¾åƒå¤„ç†å¤±è´¥"
        case .memoryError:
            return "å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•å¤„ç†è¾ƒå°çš„å›¾åƒ"
        }
    }
}

@MainActor
class CoreMLImageProcessor: ObservableObject {
    @Published var isProcessing = false
    @Published var progress: Double = 0.0
    @Published var lastError: ImageProcessingError?
    @Published var isModelLoaded = false
    
    private var models: [ImageProcessingType: MLModel] = [:]
    private var loadingTask: Task<Void, Never>?
    
    init() {
        // å¼‚æ­¥åŠ è½½æ¨¡å‹ä»¥é¿å…é˜»å¡UI
        loadingTask = Task {
            await loadModelsAsync()
            await MainActor.run {
                self.isModelLoaded = true
            }
        }
    }
    
    deinit {
        loadingTask?.cancel()
    }
    
    private func loadModelsAsync() async {
        await withTaskGroup(of: Void.self) { group in
            for type in ImageProcessingType.allCases {
                group.addTask {
                    await self.loadSingleModel(type: type)
                }
            }
        }
        
        print("Total models loaded: \(models.count)")
        print("Available models: \(models.keys.map { $0.rawValue })")
    }
    
    private func loadSingleModel(type: ImageProcessingType) async {
        do {
            // é¦–å…ˆå°è¯•ä»Bundleä¸­åŠ è½½ç¼–è¯‘åçš„æ¨¡å‹
            if let compiledModel = Bundle.main.url(forResource: type.rawValue, withExtension: "mlmodelc") {
                print("Found compiled model at: \(compiledModel)")
                
                // é…ç½®æ¨¡å‹ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„è®¡ç®—èµ„æº
                let configuration = MLModelConfiguration()
                configuration.computeUnits = .all  // ä½¿ç”¨CPU + ç¥ç»å¼•æ“ + GPU
                configuration.allowLowPrecisionAccumulationOnGPU = true  // å…è®¸GPUä½ç²¾åº¦åŠ é€Ÿ
                
                let model = try MLModel(contentsOf: compiledModel, configuration: configuration)
                await MainActor.run {
                    self.models[type] = model
                }
                print("Successfully loaded compiled model with full compute units: \(type.rawValue)")
                return
            }
            
            print("No compiled model found for: \(type.rawValue)")
            
            // å°è¯•åœ¨ä¸åŒä½ç½®æŸ¥æ‰¾åŸå§‹æ¨¡å‹æ–‡ä»¶
            var modelURL: URL?
            
            // é¦–å…ˆåœ¨æ ¹ç›®å½•æŸ¥æ‰¾
            modelURL = Bundle.main.url(forResource: type.rawValue, withExtension: "mlmodel")
            
            // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
            if modelURL == nil {
                modelURL = Bundle.main.url(forResource: type.rawValue, withExtension: "mlmodel", subdirectory: "CoreML/superresolution")
            }
            
            guard let url = modelURL else {
                print("Model file not found anywhere: \(type.rawValue)")
                return
            }
            
            print("Found original model at: \(url)")
            let compiledURL = try await MLModel.compileModel(at: url)
            
            // é…ç½®æ¨¡å‹ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„è®¡ç®—èµ„æº
            let configuration = MLModelConfiguration()
            configuration.computeUnits = .all  // ä½¿ç”¨CPU + ç¥ç»å¼•æ“ + GPU
            configuration.allowLowPrecisionAccumulationOnGPU = true  // å…è®¸GPUä½ç²¾åº¦åŠ é€Ÿ
            
            let model = try MLModel(contentsOf: compiledURL, configuration: configuration)
            await MainActor.run {
                self.models[type] = model
            }
            print("Successfully compiled and loaded model with full compute units: \(type.rawValue)")
        } catch {
            print("Failed to load model \(type.rawValue): \(error)")
        }
    }
    
    func processImage(_ image: UIImage, type: ImageProcessingType, params: SketchProcessingParams = .default) async throws -> UIImage {
        guard let model = models[type] else {
            throw ImageProcessingError.modelNotFound(type.rawValue)
        }
        
        isProcessing = true
        progress = 0.0
        lastError = nil
        
        defer {
            isProcessing = false
            progress = 0.0
        }
        
        do {
            progress = 0.05
            
            // å¯é€‰çš„æ™ºèƒ½é¢„å¤„ç†
            let enhancedImage: UIImage
            if params.enablePreprocessing {
                print("ğŸ“¸ å¯ç”¨æ™ºèƒ½é¢„å¤„ç† - ä¼˜åŒ–å›¾åƒè´¨é‡ä¸­...")
                enhancedImage = enhanceImageForModelProcessing(image, params: params)
                print("âœ… é¢„å¤„ç†å®Œæˆ - å›¾åƒå·²ä¼˜åŒ–")
            } else {
                print("âš¡ è·³è¿‡é¢„å¤„ç† - ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒ")
                enhancedImage = image
            }
            
            progress = 0.15
            
            // æ ¹æ®æ¨¡å‹ç±»å‹é¢„å¤„ç†å›¾åƒå°ºå¯¸
            let preprocessedImage = preprocessImageForModel(enhancedImage, type: type)
            
            progress = 0.25
            
            // å‡†å¤‡è¾“å…¥å›¾åƒ
            guard let pixelBuffer = preprocessedImage.toPixelBuffer() else {
                throw ImageProcessingError.invalidInput
            }
            
            progress = 0.4
            
            // å¤„ç†å›¾åƒ
            let modelResult: UIImage
            switch type {
            case .animeToSketch:
                modelResult = try await processAnimeToSketch(model: model, pixelBuffer: pixelBuffer)
            }
            
            progress = 0.8
            
            // åº”ç”¨åå¤„ç†å‚æ•°
            let finalResult = applySketchPostProcessing(modelResult, params: params)
            
            progress = 1.0
            return finalResult
            
        } catch {
            if let processingError = error as? ImageProcessingError {
                lastError = processingError
                throw processingError
            } else {
                print("Image processing failed: \(error.localizedDescription)")
                lastError = .processingFailed
                throw ImageProcessingError.processingFailed
            }
        }
    }
    
    /// é¢„å¤„ç†å›¾åƒä»¥ä¼˜åŒ–æ¨¡å‹è¾“å…¥è´¨é‡
    private func enhanceImageForModelProcessing(_ image: UIImage, params: SketchProcessingParams) -> UIImage {
        guard let cgImage = image.cgImage else { return image }
        
        let ciImage = CIImage(cgImage: cgImage)
        let context = CIContext()
        
        var processedImage = ciImage
        
        // åˆ†æåŸå§‹å›¾åƒç‰¹å¾
        let originalBrightness = calculateImageBrightness(ciImage)
        print("ğŸ” å›¾åƒåˆ†æ - äº®åº¦: \(String(format: "%.2f", originalBrightness))")
        
        // 1. ç›´æ–¹å›¾å‡è¡¡åŒ– - å¢å¼ºå¯¹æ¯”åº¦å’Œç»†èŠ‚
        if let exposureFilter = CIFilter(name: "CIExposureAdjust") {
            exposureFilter.setValue(processedImage, forKey: kCIInputImageKey)
            // æ ¹æ®å›¾åƒäº®åº¦è‡ªåŠ¨è°ƒæ•´æ›å…‰
            let exposureAdjustment = originalBrightness < 0.3 ? 0.5 : (originalBrightness > 0.7 ? -0.3 : 0.0)
            exposureFilter.setValue(exposureAdjustment, forKey: kCIInputEVKey)
            if let result = exposureFilter.outputImage {
                processedImage = result
                print("âš¡ æ›å…‰è°ƒæ•´: \(String(format: "%.2f", exposureAdjustment))")
            }
        }
        
        // 2. å±€éƒ¨å¯¹æ¯”åº¦å¢å¼º - å¢å¼ºè¾¹ç¼˜å’Œç»†èŠ‚
        if let contrastFilter = CIFilter(name: "CIColorControls") {
            contrastFilter.setValue(processedImage, forKey: kCIInputImageKey)
            // æ ¹æ®å›¾åƒç‰¹å¾è‡ªåŠ¨è°ƒæ•´å¯¹æ¯”åº¦
            let autoContrast = calculateOptimalContrast(ciImage)
            contrastFilter.setValue(autoContrast, forKey: kCIInputContrastKey)
            if let result = contrastFilter.outputImage {
                processedImage = result
            }
        }
        
        // 3. é”åŒ–å¤„ç† - å¢å¼ºè¾¹ç¼˜æ£€æµ‹æ•ˆæœ
        if let sharpenFilter = CIFilter(name: "CISharpenLuminance") {
            sharpenFilter.setValue(processedImage, forKey: kCIInputImageKey)
            sharpenFilter.setValue(0.8, forKey: kCIInputSharpnessKey)
            if let result = sharpenFilter.outputImage {
                processedImage = result
            }
        }
        
        // 4. å»å™ªå¤„ç† - å‡å°‘å™ªç‚¹å¹²æ‰°
        if let noiseReductionFilter = CIFilter(name: "CINoiseReduction") {
            noiseReductionFilter.setValue(processedImage, forKey: kCIInputImageKey)
            noiseReductionFilter.setValue(0.02, forKey: "inputNoiseLevel")
            noiseReductionFilter.setValue(0.40, forKey: "inputSharpness")
            if let result = noiseReductionFilter.outputImage {
                processedImage = result
            }
        }
        
        // 5. ä¼½é©¬æ ¡æ­£ - ä¼˜åŒ–ä¸­é—´è°ƒ
        if let gammaFilter = CIFilter(name: "CIGammaAdjust") {
            gammaFilter.setValue(processedImage, forKey: kCIInputImageKey)
            // æ ¹æ®å›¾åƒç‰¹å¾è°ƒæ•´ä¼½é©¬å€¼
            let gamma = calculateOptimalGamma(ciImage)
            gammaFilter.setValue(gamma, forKey: "inputPower")
            if let result = gammaFilter.outputImage {
                processedImage = result
            }
        }
        
        // 6. é˜´å½±é«˜å…‰è°ƒæ•´ - ä¿ç•™ç»†èŠ‚
        if let shadowHighlightFilter = CIFilter(name: "CIShadowHighlight") {
            shadowHighlightFilter.setValue(processedImage, forKey: kCIInputImageKey)
            shadowHighlightFilter.setValue(0.3, forKey: "inputShadowAmount")  // æäº®é˜´å½±
            shadowHighlightFilter.setValue(-0.2, forKey: "inputHighlightAmount")  // é™ä½é«˜å…‰
            if let result = shadowHighlightFilter.outputImage {
                processedImage = result
            }
        }
        
        // è½¬æ¢å›UIImage
        guard let finalCGImage = context.createCGImage(processedImage, from: processedImage.extent) else {
            return image
        }
        
        return UIImage(cgImage: finalCGImage)
    }
    
    /// è®¡ç®—å›¾åƒäº®åº¦
    private func calculateImageBrightness(_ image: CIImage) -> Float {
        let extent = image.extent
        let inputKeys = [kCIInputImageKey: image, kCIInputExtentKey: CIVector(cgRect: extent)]
        
        guard let filter = CIFilter(name: "CIAreaAverage", parameters: inputKeys),
              let outputImage = filter.outputImage else {
            return 0.5 // é»˜è®¤ä¸­ç­‰äº®åº¦
        }
        
        let context = CIContext()
        var bitmap = [UInt8](repeating: 0, count: 4)
        context.render(outputImage, toBitmap: &bitmap, rowBytes: 4, bounds: CGRect(x: 0, y: 0, width: 1, height: 1), format: .RGBA8, colorSpace: nil)
        
        // è®¡ç®—æ„ŸçŸ¥äº®åº¦
        let r = Float(bitmap[0]) / 255.0
        let g = Float(bitmap[1]) / 255.0
        let b = Float(bitmap[2]) / 255.0
        
        return 0.299 * r + 0.587 * g + 0.114 * b
    }
    
    /// è®¡ç®—æœ€ä¼˜å¯¹æ¯”åº¦
    private func calculateOptimalContrast(_ image: CIImage) -> Float {
        let brightness = calculateImageBrightness(image)
        
        // æ ¹æ®äº®åº¦è‡ªé€‚åº”è°ƒæ•´å¯¹æ¯”åº¦
        if brightness < 0.3 {
            return 1.4  // æš—å›¾å¢å¼ºå¯¹æ¯”åº¦
        } else if brightness > 0.7 {
            return 1.1  // äº®å›¾è½»å¾®å¢å¼º
        } else {
            return 1.2  // ä¸­ç­‰äº®åº¦é€‚ä¸­å¢å¼º
        }
    }
    
    /// è®¡ç®—æœ€ä¼˜ä¼½é©¬å€¼
    private func calculateOptimalGamma(_ image: CIImage) -> Float {
        let brightness = calculateImageBrightness(image)
        
        // æ ¹æ®äº®åº¦è°ƒæ•´ä¼½é©¬å€¼
        if brightness < 0.3 {
            return 0.8  // æš—å›¾é™ä½ä¼½é©¬ï¼Œæäº®ä¸­é—´è°ƒ
        } else if brightness > 0.7 {
            return 1.2  // äº®å›¾æé«˜ä¼½é©¬ï¼Œå‹æš—ä¸­é—´è°ƒ
        } else {
            return 1.0  // ä¸­ç­‰äº®åº¦ä¿æŒåŸæ ·
        }
    }
    
    private func applySketchPostProcessing(_ image: UIImage, params: SketchProcessingParams) -> UIImage {
        guard let cgImage = image.cgImage else { return image }
        
        let ciImage = CIImage(cgImage: cgImage)
        let context = CIContext()
        
        var outputImage = ciImage
        
        // 1. è°ƒæ•´å¯¹æ¯”åº¦
        if params.contrast != 1.0 {
            let contrastFilter = CIFilter(name: "CIColorControls")!
            contrastFilter.setValue(outputImage, forKey: kCIInputImageKey)
            contrastFilter.setValue(params.contrast, forKey: kCIInputContrastKey)
            if let result = contrastFilter.outputImage {
                outputImage = result
            }
        }
        
        // 2. è°ƒæ•´äº®åº¦
        if params.brightness != 0.0 {
            let brightnessFilter = CIFilter(name: "CIColorControls")!
            brightnessFilter.setValue(outputImage, forKey: kCIInputImageKey)
            brightnessFilter.setValue(params.brightness, forKey: kCIInputBrightnessKey)
            if let result = brightnessFilter.outputImage {
                outputImage = result
            }
        }
        
        // 3. å¢å¼ºçº¿æ¡ï¼ˆä½¿ç”¨é”åŒ–ï¼‰
        if params.lineIntensity != 1.0 {
            let sharpenFilter = CIFilter(name: "CISharpenLuminance")!
            sharpenFilter.setValue(outputImage, forKey: kCIInputImageKey)
            sharpenFilter.setValue(params.lineIntensity, forKey: kCIInputSharpnessKey)
            if let result = sharpenFilter.outputImage {
                outputImage = result
            }
        }
        
        // 4. èƒŒæ™¯ç™½åŒ–å¤„ç†
        if params.backgroundWhiteness > 0.0 {
            let exposureFilter = CIFilter(name: "CIExposureAdjust")!
            exposureFilter.setValue(outputImage, forKey: kCIInputImageKey)
            exposureFilter.setValue(params.backgroundWhiteness, forKey: kCIInputEVKey)
            if let result = exposureFilter.outputImage {
                outputImage = result
            }
        }
        
        // è½¬æ¢å›UIImage
        guard let finalCGImage = context.createCGImage(outputImage, from: outputImage.extent) else {
            return image
        }
        
        return UIImage(cgImage: finalCGImage)
    }
    
    private func preprocessImageForModel(_ image: UIImage, type: ImageProcessingType) -> UIImage {
        switch type {
        case .animeToSketch:
            // AnimeToSketch éœ€è¦ 512x512ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……é¿å…ç©ºç™½åŒºåŸŸ
            let targetSize = CGSize(width: 512, height: 512)
            print("AnimeToSketch - Original size: \(image.size), Target size: \(targetSize)")
            return image.resizedWithPadding(to: targetSize)
        }
    }
    
    private func processAnimeToSketch(model: MLModel, pixelBuffer: CVPixelBuffer) async throws -> UIImage {
        progress = 0.5
        
        // æ‰“å°æ¨¡å‹ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
        print("AnimeToSketch model inputs: \(model.modelDescription.inputDescriptionsByName.keys)")
        print("AnimeToSketch model outputs: \(model.modelDescription.outputDescriptionsByName.keys)")
        
        // å°è¯•ä¸åŒçš„è¾“å…¥å‚æ•°å
        var input: MLDictionaryFeatureProvider
        
        // é¦–å…ˆå°è¯• "input_1"ï¼ˆä»é”™è¯¯ä¿¡æ¯çœ‹è¿™æ˜¯å¿…éœ€çš„ï¼‰
        if let _ = model.modelDescription.inputDescriptionsByName["input_1"] {
            input = try MLDictionaryFeatureProvider(dictionary: ["input_1": MLFeatureValue(pixelBuffer: pixelBuffer)])
        } else if let _ = model.modelDescription.inputDescriptionsByName["input"] {
            input = try MLDictionaryFeatureProvider(dictionary: ["input": MLFeatureValue(pixelBuffer: pixelBuffer)])
        } else {
            // å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å…¥
            let firstInputName = model.modelDescription.inputDescriptionsByName.keys.first ?? "input"
            input = try MLDictionaryFeatureProvider(dictionary: [firstInputName: MLFeatureValue(pixelBuffer: pixelBuffer)])
            print("Using input name: \(firstInputName)")
        }
        
        progress = 0.7
        
        let output = try await model.prediction(from: input)
        
        progress = 0.9
        
        // å°è¯•ä¸åŒçš„è¾“å‡ºå‚æ•°å
        var outputBuffer: CVPixelBuffer?
        
        if let buffer = output.featureValue(for: "output")?.imageBufferValue {
            outputBuffer = buffer
        } else if let buffer = output.featureValue(for: "output_1")?.imageBufferValue {
            outputBuffer = buffer
        } else {
            // ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡º
            let firstOutputName = model.modelDescription.outputDescriptionsByName.keys.first ?? "output"
            outputBuffer = output.featureValue(for: firstOutputName)?.imageBufferValue
            print("Using output name: \(firstOutputName)")
        }
        
        guard let buffer = outputBuffer else {
            throw ImageProcessingError.processingFailed
        }
        
        guard let resultImage = UIImage(pixelBuffer: buffer) else {
            throw ImageProcessingError.processingFailed
        }
        
        return resultImage
    }
    
    
    func isModelAvailable(_ type: ImageProcessingType) -> Bool {
        return models[type] != nil
    }
    
    func availableModels() -> [ImageProcessingType] {
        return ImageProcessingType.allCases.filter { isModelAvailable($0) }
    }
}

// MARK: - UIImage Extensions
extension UIImage {
    func resized(to size: CGSize) -> UIImage {
        let renderer = UIGraphicsImageRenderer(size: size)
        return renderer.image { _ in
            self.draw(in: CGRect(origin: .zero, size: size))
        }
    }
    
    /// æ™ºèƒ½å¡«å……è°ƒæ•´ï¼šä¿æŒå®½é«˜æ¯”ï¼Œä½¿ç”¨è¾¹ç¼˜åƒç´ å¡«å……ç©ºç™½åŒºåŸŸ
    func resizedWithPadding(to targetSize: CGSize) -> UIImage {
        let sourceSize = self.size
        let sourceAspectRatio = sourceSize.width / sourceSize.height
        let targetAspectRatio = targetSize.width / targetSize.height
        
        // è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼Œç¡®ä¿å›¾åƒå®Œå…¨æ˜¾ç¤ºï¼‰
        let scaledSize: CGSize
        if sourceAspectRatio > targetAspectRatio {
            // æºå›¾åƒæ›´å®½ï¼Œä»¥å®½åº¦ä¸ºå‡†
            scaledSize = CGSize(width: targetSize.width, height: targetSize.width / sourceAspectRatio)
        } else {
            // æºå›¾åƒæ›´é«˜ï¼Œä»¥é«˜åº¦ä¸ºå‡†
            scaledSize = CGSize(width: targetSize.height * sourceAspectRatio, height: targetSize.height)
        }
        
        // è®¡ç®—å›¾åƒåœ¨ç›®æ ‡ç”»å¸ƒä¸­çš„ä½ç½®ï¼ˆå±…ä¸­ï¼‰
        let x = (targetSize.width - scaledSize.width) / 2
        let y = (targetSize.height - scaledSize.height) / 2
        
        let renderer = UIGraphicsImageRenderer(size: targetSize)
        return renderer.image { context in
            let cgContext = context.cgContext
            
            // é¦–å…ˆç”¨è¾¹ç¼˜åƒç´ é¢œè‰²å¡«å……æ•´ä¸ªç”»å¸ƒ
            if let cgImage = self.cgImage {
                // è·å–è¾¹ç¼˜åƒç´ é¢œè‰²ä½œä¸ºèƒŒæ™¯è‰²
                let backgroundColor = self.getEdgeColor()
                cgContext.setFillColor(backgroundColor)
                cgContext.fill(CGRect(origin: .zero, size: targetSize))
                
                // ç„¶åç»˜åˆ¶ç¼©æ”¾åçš„å›¾åƒ
                self.draw(in: CGRect(x: x, y: y, width: scaledSize.width, height: scaledSize.height))
            } else {
                // å¦‚æœæ— æ³•è·å–CGImageï¼Œä½¿ç”¨ç°è‰²èƒŒæ™¯
                cgContext.setFillColor(UIColor.gray.cgColor)
                cgContext.fill(CGRect(origin: .zero, size: targetSize))
                self.draw(in: CGRect(x: x, y: y, width: scaledSize.width, height: scaledSize.height))
            }
        }
    }
    
    /// è·å–å›¾åƒè¾¹ç¼˜çš„å¹³å‡é¢œè‰²
    private func getEdgeColor() -> CGColor {
        guard let cgImage = self.cgImage else {
            return UIColor.gray.cgColor
        }
        
        let width = cgImage.width
        let height = cgImage.height
        
        guard width > 0 && height > 0 else {
            return UIColor.gray.cgColor
        }
        
        // åˆ›å»º1x1çš„ä¸Šä¸‹æ–‡æ¥è·å–å¹³å‡é¢œè‰²
        let colorSpace = CGColorSpaceCreateDeviceRGB()
        let bitmapInfo = CGImageAlphaInfo.premultipliedLast.rawValue
        
        guard let context = CGContext(
            data: nil,
            width: 1,
            height: 1,
            bitsPerComponent: 8,
            bytesPerRow: 4,
            space: colorSpace,
            bitmapInfo: bitmapInfo
        ) else {
            return UIColor.gray.cgColor
        }
        
        // ç»˜åˆ¶æ•´ä¸ªå›¾åƒåˆ°1x1ä¸Šä¸‹æ–‡ä¸­ï¼Œå¾—åˆ°å¹³å‡é¢œè‰²
        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: 1, height: 1))
        
        guard let data = context.data else {
            return UIColor.gray.cgColor
        }
        
        let pixelData = data.assumingMemoryBound(to: UInt8.self)
        let red = CGFloat(pixelData[0]) / 255.0
        let green = CGFloat(pixelData[1]) / 255.0
        let blue = CGFloat(pixelData[2]) / 255.0
        let alpha = CGFloat(pixelData[3]) / 255.0
        
        return UIColor(red: red, green: green, blue: blue, alpha: alpha).cgColor
    }
    
    /// ä¸­å¿ƒè£å‰ªåˆ°256Ã—256ï¼Œä¿æŒé¢œè‰²ä¸å˜ï¼Œé¿å…KMEMé”™è¯¯
    func resizedToFit512x512() -> UIImage {
        let targetSize = CGSize(width: 256, height: 256)  // ä½¿ç”¨256é¿å…å†…å­˜é—®é¢˜
        let sourceSize = self.size
        
        // è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆå–è¾ƒå¤§çš„æ¯”ä¾‹ä»¥å¡«æ»¡ç›®æ ‡å°ºå¯¸ï¼‰
        let scale = max(targetSize.width / sourceSize.width, targetSize.height / sourceSize.height)
        
        // è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
        let scaledSize = CGSize(
            width: sourceSize.width * scale,
            height: sourceSize.height * scale
        )
        
        // è®¡ç®—è£å‰ªåŒºåŸŸï¼ˆå±…ä¸­ï¼‰
        let cropRect = CGRect(
            x: (scaledSize.width - targetSize.width) / 2,
            y: (scaledSize.height - targetSize.height) / 2,
            width: targetSize.width,
            height: targetSize.height
        )
        
        let renderer = UIGraphicsImageRenderer(size: targetSize)
        return renderer.image { _ in
            // å…ˆå°†å›¾åƒç»˜åˆ¶åˆ°æ”¾å¤§çš„å°ºå¯¸
            let drawRect = CGRect(
                x: -cropRect.minX,
                y: -cropRect.minY,
                width: scaledSize.width,
                height: scaledSize.height
            )
            self.draw(in: drawRect)
        }
    }
    
    /// è°ƒæ•´åˆ°æŒ‡å®šå°ºå¯¸ï¼Œä½¿ç”¨ä¸­å¿ƒè£å‰ª
    func resizedToSpecificSize(_ targetSize: CGSize) -> UIImage {
        let sourceSize = self.size
        
        // è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆå–è¾ƒå¤§çš„æ¯”ä¾‹ä»¥å¡«æ»¡ç›®æ ‡å°ºå¯¸ï¼‰
        let scale = max(targetSize.width / sourceSize.width, targetSize.height / sourceSize.height)
        
        // è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
        let scaledSize = CGSize(
            width: sourceSize.width * scale,
            height: sourceSize.height * scale
        )
        
        // è®¡ç®—è£å‰ªåŒºåŸŸï¼ˆå±…ä¸­ï¼‰
        let cropRect = CGRect(
            x: (scaledSize.width - targetSize.width) / 2,
            y: (scaledSize.height - targetSize.height) / 2,
            width: targetSize.width,
            height: targetSize.height
        )
        
        let renderer = UIGraphicsImageRenderer(size: targetSize)
        return renderer.image { _ in
            // å…ˆå°†å›¾åƒç»˜åˆ¶åˆ°æ”¾å¤§çš„å°ºå¯¸
            let drawRect = CGRect(
                x: -cropRect.minX,
                y: -cropRect.minY,
                width: scaledSize.width,
                height: scaledSize.height
            )
            self.draw(in: drawRect)
        }
    }
    
    func toPixelBuffer() -> CVPixelBuffer? {
        let width = Int(self.size.width)
        let height = Int(self.size.height)
        
        var pixelBuffer: CVPixelBuffer?
        let attributes: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true,
            kCVPixelBufferMetalCompatibilityKey: true
        ]
        
        let status = CVPixelBufferCreate(
            nil,
            width,
            height,
            kCVPixelFormatType_32BGRA,
            attributes as CFDictionary,
            &pixelBuffer
        )
        
        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }
        
        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }
        
        let context = CGContext(
            data: CVPixelBufferGetBaseAddress(buffer),
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        )
        
        guard let cgContext = context, let cgImage = self.cgImage else {
            return nil
        }
        
        cgContext.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        
        return buffer
    }
    
    convenience init?(pixelBuffer: CVPixelBuffer) {
        var cgImage: CGImage?
        VTCreateCGImageFromCVPixelBuffer(pixelBuffer, options: nil, imageOut: &cgImage)
        
        guard let cgImg = cgImage else {
            return nil
        }
        
        self.init(cgImage: cgImg)
    }
}
